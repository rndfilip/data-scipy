{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize with SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëã Say hello to [SciPy](https://scipy.org/) (`Scientific Python`), a powerful library:\n",
    "- based on NumPy\n",
    "- used for Mathematics in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Local Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In our previous exercise, we saw that it is important to **minimize the (mean) squared errors between the targets and the predictions** by  finding the minimum of a given function.\n",
    "\n",
    "* We have two approaches for this problem:\n",
    "    1. `closed-formed solutions` (such as _matrix inversion_)\n",
    "    2. `iterative approaches` (such as _gradient descent_)\n",
    "\n",
    "üëâ Let's discover how to use the [`scipy.optimize`](https://docs.scipy.org/doc/scipy/tutorial/optimize.html) module to find the **local** minimium of a function _(in a few lines of code)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import optimize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) 1D-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the function $ f : x \\rightarrow x^2 - 20 \\cdot  cos(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x**2 - 20 * np.cos(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot this function with the *x-axis* below between -10 and +10 ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Try to find the **local minimum of $f$**, from a starting point `x0`, using `optimize.minimize()` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Starting from a given point $ x_0 $, `optimize.minimize()` was able to find:\n",
    "* a **miminum of $f$** near a value displayed in the variable `x`\n",
    "* and gives you the number of iterations `nit` it took for `scipy` to converge the minimum of the function $f$.\n",
    "\n",
    "What can you conclude ‚ùì\n",
    "\n",
    "‚ùì Try to change the `x0` and observe what is happening ‚ùì\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary><i>Explanations</i></summary>\n",
    "    \n",
    "üí° After <code>nit</code> iterations, the algorithm of the <code>scipy.minimize()</code> function gets stuck in a local minimum <code>x</code>, except if it started from a value of <code>x0</code> that was already close enough to the global minimum (which is 0 according to the graph above). The value of the local minimum found can be read in  <code>fun</code> - 15.79 in our case.\n",
    "    \n",
    "üí° In math, we say that this function is not [convex](https://en.wikipedia.org/wiki/Convex_function). If the function was `convex`, any local minimum would actually be the global minimum! In fact, Machine-Learning loves convexity, and such problems with convex functions are very easy to solve with iterative processes such as the gradient descent!\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2)  2D-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the previous exercise, finding the minimum of a function with more than one parameter becomes rapidly complex. Let's try it out with a multivariate function: $$ g : (x,y) \\rightarrow -(y + 47) \\cdot   sin(\\sqrt{\\lvert\\frac{x}{2} + (y  + 47) \\rvert} ) \\\n",
    "        -x \\cdot sin(\\sqrt{\\lvert x - (y  + 47)\\rvert}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(X):\n",
    "    return -(X[1] + 47) * np.sin(np.sqrt(abs(X[0]/2 + (X[1]  + 47)))) \\\n",
    "        -X[0] * np.sin(np.sqrt(abs(X[0] - (X[1]  + 47))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Run the cell below to visualize `g` in 3D ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a (50*50) meshgrid\n",
    "x = np.linspace(-150,150,100) # shape(100,1)\n",
    "y = np.linspace(-150,150,100) # shape(100,1)\n",
    "xx, yy = np.meshgrid(x,y) # x and y of shape(100,100)\n",
    "zz = np.array([xx, yy]) # (2, 100, 100)\n",
    "\n",
    "# Compute Z, a 2D-array containing g(x,y) for each (x,y) in the meshgrid\n",
    "Z = g(zz)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xx, yy, Z, cmap='terrain')\n",
    "ax.set_xlabel('x'); ax.set_ylabel('y'); ax.set_zlabel('g(x, y)'); ax.view_init(45, -45);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìInitialize a starting point `X0` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìFind a `minimum` using `optimize.minimize()` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and store the minimum inside a `minimum` variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìDoes this look like the absolute minimum ‚ùì Check it out below ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out\n",
    "\n",
    "x = np.linspace(-150,150,100) # shape(100,1)\n",
    "y = np.linspace(-150,150,100) # shape(100,1)\n",
    "xx, yy = np.meshgrid(x,y) # x and y of shape(100,100)\n",
    "zz = np.array([xx,yy]) # shape(2, 100, 100)\n",
    "\n",
    "plt.contourf(xx,yy,g(zz), 40)\n",
    "plt.colorbar()\n",
    "plt.scatter(minimum[0], minimum[1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è You probably only found a **local** minimum to your objective function f, given a starting point $X0$.\n",
    "\n",
    "‚ùì Can you think about  a procedure that would increase your chance of finding the **global** minima ‚ùì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üëâ One empirical idea could be to loop over many random starting points $X0$, and store the minimum value found at each run. After each iteration, you should increase your chance of finding the global minimum (if there is any) üëà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß™Check your code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('minimize2d',\n",
    "    X0_shape=X0.shape,\n",
    "    minimum_shape=minimum.shape\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Minimize under constraint(s) üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ In real world problems, we often want to minimize a function $h(\\textbf{x})$, **given a set of constraints on the values of $\\textbf{x}$ itself**.   \n",
    "\n",
    "üéí See for instance, the famous [Knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem) \n",
    "\n",
    "üë©üèª‚Äçüè´ As often in Mathematics, **the hardest part is not to solve the equations but to convert your real-world problem into mathematical equations**. But for the sake of this challenge, let's assume we came with the following problem statement:\n",
    "\n",
    "---\n",
    "Find $\\textbf{x}$ that minimizes $h(\\textbf{x}) = x_1 x_4 (x_1 + x_2 + x_3) + x_3$  \n",
    "\n",
    "Given the following constraints\n",
    "\n",
    "\n",
    "$[1]\\ \\ x_1^2 + x_2^2 + x_3^2 + x_4^2 = 40$  (*equality constraint*)\n",
    "\n",
    "$[2]\\ \\ x_1 x_2 x_3 x_4 \\leqslant 25$ (*inequality constraint*)\n",
    "\n",
    "$[3]\\ \\ 1 \\leqslant x_1, x_2, x_3, x_4 \\leqslant 5$ (*bounds*)\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Re-use the `minimize` method to find a local minimum using additional arguments as follows: \n",
    "\n",
    "`optimize.minimize(h, X0, constraints=cons, bounds=boundaries)` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your objective function h(X) that you want to minimize\n",
    "# X being a 1D-array of length 4\n",
    "\n",
    "def h(X):\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function constraint1(X) that returns 0 if and only if equation [1] is True\n",
    "# Otherwise it should return any other number\n",
    "\n",
    "def constraint1(X):\n",
    "    pass  # YOUR CODE HERE\n",
    "\n",
    "# Define a function constraint2(X) that returns a positive number if and only if equation [2] is True\n",
    "# Otherwise it should return a negative number \n",
    "def constraint2(X):\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Hint</i></summary>\n",
    "    The above functions do not require an if else statement if we convert our constraints to Math equations.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ We are now defining for you the \"constraint\" argument needed to minimize function. Pay attention to the [`optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1 = {\"type\": \"eq\", \"fun\": constraint1}\n",
    "con2 = {\"type\": \"ineq\", \"fun\": constraint2}\n",
    "constraints = [con1, con2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Let's deal with equation [3].\n",
    "We could write them in the form of 10 constraint functions, but it would take too much time.\n",
    "\n",
    "üëâ Instead, `scipy` allows us to create \"boundaries\" arguments for the variables we are looking for, in the following form:  \n",
    "`bounds` = tuple of tuple `((x1_min, x_1_max), (x2_min, x_2_max), ....)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, define any starting point X0 for the minimization algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now, try to find the minimum of your objective function `h` under such constraints using `optimize.minimize` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize under constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the minimum in a `local_minimum` variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß™Check your code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('minimize_constraints',\n",
    "    bounds=bounds,\n",
    "    X0=X0,\n",
    "    Xmin=local_minimum\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Global Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, `optimize.minimize` finds **the local minima only in the vicinity of X0**. To find the **global** minima, you basically have two options:\n",
    "\n",
    "<u> Option 1</u>: For **Math Nerds only** ü§ì\n",
    "\n",
    "Prove mathematically that your optimization problem is geometrically [convex](https://en.wikipedia.org/wiki/Convex_function). An optimization problem is convex in the following case: \n",
    "1. its objective function `h` is a convex function, \n",
    "2. the inequality constraints are convex, and \n",
    "3. the equality constraints are affine. \n",
    "    \n",
    "üìö Read in this excellent math-based presentation from Berkeley if you want to dig further: [Convex Optimization for Machine Learning](https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/optimization/slides.pdf)\n",
    "\n",
    "\n",
    "<u>Option 2</u>: For **pragmatic people** üòÅ\n",
    "\n",
    "- Loop over many different starting points $X0$ and look for the local minima nearby. \n",
    "- Store the minimum value found at each run. \n",
    "- After each iteration, you increase your chance of finding the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "üí° <u>Option 2</u> is exactly what the following Scipy libraries do under the hood: they _efficiently_ search for the parameter space, while using `minimize` at each iteration. It works better when the number of parameters to search for (degree of freedom) is small ([`scipy.optimize.shgo`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.shgo.html#scipy.optimize.shgo) or [`scipy.optimize.dual_annealing`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.dual_annealing.html))\n",
    "\n",
    "‚ùóÔ∏è However, they only return a \"global\" minima **within specified boundaries** for the parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìTry to use the two functions to find the global minima for our previous 2D-function $g(x,y)$, bounded between -150 and +150 ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable `bounds` with boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the global minimum `minimum_shgo` with the optimize.shgo method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out\n",
    "\n",
    "x = np.linspace(-150,150,100) # shape(100,1)\n",
    "y = np.linspace(-150,150,100) # shape(100,1)\n",
    "xx, yy = np.meshgrid(x,y) # x and y of shape(100,100)\n",
    "zz = np.array([xx,yy]) # shape(2, 100, 100)\n",
    "\n",
    "plt.contourf(xx,yy,g(zz), 40)\n",
    "plt.colorbar()\n",
    "plt.scatter(minimum_shgo[0], minimum_shgo[1], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the global minimum `minimum_dual` with the optimize.dual_annealing method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out\n",
    "\n",
    "x = np.linspace(-150,150,100) # shape(100,1)\n",
    "y = np.linspace(-150,150,100) # shape(100,1)\n",
    "xx, yy = np.meshgrid(x,y) # x and y of shape(100,100)\n",
    "zz = np.array([xx,yy]) # shape(2, 100, 100)\n",
    "\n",
    "plt.contourf(xx,yy,g(zz), 40)\n",
    "plt.colorbar()\n",
    "plt.scatter(minimum_dual[0], minimum_dual[1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß™Check your code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('global_optimization',\n",
    "    bounds=bounds,\n",
    "    Xmin_shgo=minimum_shgo,\n",
    "    Xmin_dual=minimum_dual\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Aproximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1) Fitting a scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßëüèª‚Äçüè´A preliminary step to analyze the relationship between two quantitative variables X and Y is to **scatterplot** them. \n",
    "\n",
    "ü§îThen, the question is: what relation do we have between X and Y ? Can we fit a scatterplot with a straight line ? If not, can we approximate the relationship between X and Y by a polynomial curve ? a logarithm curve ? ...\n",
    "\n",
    "We often have to fit a scatterplot with a straight line, but it can also happen to look like something else (polynomial, logarithmic etc...)\n",
    "\n",
    "‚ùì Consider the dataset below: would you try to fit a linear regression curve to it ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-3.        , -2.87755102, -2.75510204, -2.63265306, -2.51020408,\n",
    "       -2.3877551 , -2.26530612, -2.14285714, -2.02040816, -1.89795918,\n",
    "       -1.7755102 , -1.65306122, -1.53061224, -1.40816327, -1.28571429,\n",
    "       -1.16326531, -1.04081633, -0.91836735, -0.79591837, -0.67346939,\n",
    "       -0.55102041, -0.42857143, -0.30612245, -0.18367347, -0.06122449,\n",
    "        0.06122449,  0.18367347,  0.30612245,  0.42857143,  0.55102041,\n",
    "        0.67346939,  0.79591837,  0.91836735,  1.04081633,  1.16326531,\n",
    "        1.28571429,  1.40816327,  1.53061224,  1.65306122,  1.7755102 ,\n",
    "        1.89795918,  2.02040816,  2.14285714,  2.26530612,  2.3877551 ,\n",
    "        2.51020408,  2.63265306,  2.75510204,  2.87755102,  3.        ])\n",
    "y = np.array([31.66815357, 31.26229494, 30.3467807 , 28.2057809 , 25.47674964,\n",
    "       22.81398414, 19.93953021, 19.38250362, 20.02551935, 17.44468883,\n",
    "       17.80733403, 16.29808282, 14.85006259, 12.69760597, 13.04075803,\n",
    "       10.42420089,  7.91118094,  9.72737214,  9.05962483,  6.89984054,\n",
    "        8.15068899,  5.15772899,  7.65448235,  4.95987628,  4.4284636 ,\n",
    "        3.22183541,  3.05456124,  3.49253584,  2.23478284,  4.15163314,\n",
    "        3.68063488,  5.22556445,  2.47139029,  2.66785497,  3.72557952,\n",
    "        2.56255802,  4.61385762,  4.28234911,  4.91138639,  5.31724926,\n",
    "        6.52053679,  5.94175001,  7.5368359 ,  9.78905172,  9.5795072 ,\n",
    "       10.95610291, 11.73051576, 12.85008617, 12.2184079 , 16.52977769])\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ In this case, a **polynomial estimator of degree 2** ( $ Y = a X^2 + b X + c) $ seems more appropriate than a simple straight line (polynomial estimator of degree 1 : $ Y = a X + b $)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Define the function $\\large f$ of degree two with parameters $ \\large (a,b,c) $ ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,a,b,c):\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ To approximate this scatterplot, we will use a polynomial function of degree 2. Hence, you need to **find the best params $(a,b,c)$**. \n",
    "\n",
    "‚ùå We could again re-use the `optimize.minimize` method to minimize the Mean Square errors between our estimator $f$ and our scatter plot but...\n",
    "\n",
    "‚úÖ Luckily, [`optimize.curve_fit`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) does this optimization process for us in just in one line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out\n",
    "optimize.curve_fit(f, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first array contains the coefficients $(a,b,c)$ that have been computed to minimize squared errors between $f$ and the dataset.\n",
    "\n",
    "\n",
    "‚ùìPlot your quadratic estimator on top of the scatter plot to check that it fits! ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.2) Interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ As with any spreadsheet software, we often want to quickly fill the blanks in a series of datapoints. We'll use [`scipy.interpolate`](https://docs.scipy.org/doc/scipy/reference/interpolate.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Let's consider the following scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 10)\n",
    "y = np.array([ 0.        ,  0.8961922 ,  0.79522006, -0.19056796, -0.96431712,\n",
    "       -0.66510151,  0.37415123,  0.99709789,  0.51060568, -0.54402111])\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí°We can use the [`interpolate.interp1d()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html) method to create a continuous function computing the image of any value $x$ in this range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_interpolated = interpolate.interp1d(x,y, kind='linear')\n",
    "f_interpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now, you can call your interpolated function with any continuous values for $x$ in the initial range. Visualize it with a new plot and a dense `linspace` for $x$. Feel free to try other `kind` of interpolations such as `quadratic` or `cubic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
